#!/usr/bin/env python3

"""The pgboom utility manipulates Postgres metadata.

For details, see https://github.com/filiprem/pg-tools/tree/master/pgboom.
"""

import argparse
import doctest
import logging
import pathlib
import re
import subprocess
import sys
import tempfile

import psycopg2
import psycopg2.errorcodes
import psycopg2.extensions

PROGNAME = "pgboom"

ACTIONS = ("explode", "implode", "diff", "cat", "test")

IGNORED_SCHEMAS_REGEX = "^(information_schema|pg_temp|pg_toast|pg_catalog|_timescaledb_|timescaledb_information)"

# Rules for producing object definitions:
# * 'list' query must produce nspname and objname.
# * full schema names must be included.
# * 'create' query must include trailing semicolon.
OBJECTMETA = {
    "SCHEMA": {
        "list": f"""SELECT n.nspname, n.nspname AS objname, n.oid
        FROM pg_catalog.pg_namespace n
        WHERE n.nspname !~ '{IGNORED_SCHEMAS_REGEX}' """,
        "create": """SELECT pg_catalog.format('CREATE SCHEMA %I;', n.nspname)
        FROM pg_catalog.pg_namespace n WHERE n.oid = __OID__ """,
    },
    "EXTENSION": {
        "list": """SELECT n.nspname, x.extname AS objname, x.oid
        FROM pg_extension x
        JOIN pg_catalog.pg_namespace n ON n.oid = x.extnamespace
        WHERE x.extname <> 'plpgsql' """,
        "create": """SELECT pg_catalog.format(
            'CREATE EXTENSION %I SCHEMA %I CASCADE;', x.extname, n.nspname)
        FROM pg_catalog.pg_extension x
        JOIN pg_catalog.pg_namespace n ON n.oid = x.extnamespace
        WHERE x.oid = __OID__""",
    },
    "FUNCTION": {  # all functions except aggregate
        "list": f"""SELECT n.nspname, p.proname || '(' ||
        array_to_string(array(select (select t.typname from pg_type t where t.oid = a.oid) from unnest(p.proargtypes) a(oid)),',')
        || ')' AS objname, p.oid
        FROM pg_proc p, pg_namespace n
        WHERE n.oid = p.pronamespace
        AND p.prokind <> 'a'
        AND n.nspname !~ '{IGNORED_SCHEMAS_REGEX}'
        AND p.oid NOT IN (SELECT objid FROM pg_depend
                 WHERE refclassid = 'pg_extension'::regclass AND deptype = 'e') """,
        "create": """SELECT regexp_replace(pg_get_functiondef(__OID__),
                 '^CREATE OR REPLACE FUNCTION', 'CREATE FUNCTION') || ';'""",
    },
    "AGGREGATE": {  # aggregate functions only
        "list": f"""SELECT n.nspname, p.proname || '(' ||
        array_to_string(array(select (select t.typname from pg_type t where t.oid = a.oid) from unnest(p.proargtypes) a(oid)),',')
        || ')' AS objname, p.oid
        FROM pg_proc p, pg_namespace n
        WHERE n.oid = p.pronamespace
        AND p.prokind = 'a'
        AND n.nspname !~ '{IGNORED_SCHEMAS_REGEX}'
        AND p.oid NOT IN (SELECT objid FROM pg_depend
                 WHERE refclassid = 'pg_extension'::regclass AND deptype = 'e') """,
        # this is overriden later (see getaggdef method).
        # we just anticipate future built-in function name here.
        "create": """SELECT pg_get_aggregatedef(__OID__) || ';'""",
    },
    "SEQUENCE": {
        "list": f"""SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n
        WHERE n.oid = c.relnamespace AND c.relkind = 'S'
        AND n.nspname !~ '{IGNORED_SCHEMAS_REGEX}'
        AND c.oid NOT IN (SELECT objid FROM pg_depend
                 WHERE refclassid = 'pg_extension'::regclass AND deptype = 'e') """,
        "create": """SELECT format(E'CREATE SEQUENCE %I.%I AS %s\n'
                'INCREMENT %s MINVALUE %s MAXVALUE %s START %s CACHE %s%s;',
            n.nspname, c.relname, pg_catalog.format_type(sp.data_type, NULL),
            sp.increment, sp.minimum_value, sp.maximum_value, sp.start_value,
            sp.cache_size, case when sp.cycle_option then ' CYCLE' else '' end)
        FROM pg_catalog.pg_class c, pg_namespace n, pg_sequence_parameters(c.oid) sp
        WHERE c.oid = __OID__ AND n.oid = c.relnamespace """,
    },
    "PARTITIONED_TABLE": {
        "list": f"""SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n
        WHERE n.oid = c.relnamespace AND c.relkind = 'p'
        AND n.nspname !~ '{IGNORED_SCHEMAS_REGEX}'
        AND c.oid NOT IN (SELECT objid FROM pg_depend
                 WHERE refclassid = 'pg_extension'::regclass AND deptype = 'e') """,
        # this is overriden later (see gettabledef method).
        # we just anticipate future built-in function name here.
        "create": """SELECT pg_get_tabledef(__OID__) || ';'""",
    },
    "TABLE": {
        "list": f"""SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n
        WHERE n.oid = c.relnamespace AND c.relkind = 'r'
        AND n.nspname !~ '{IGNORED_SCHEMAS_REGEX}'
        AND c.oid NOT IN (SELECT objid FROM pg_depend
                 WHERE refclassid = 'pg_extension'::regclass AND deptype = 'e') """,
        # this is overriden later (see gettabledef method).
        # we just anticipate future built-in function name here.
        "create": """SELECT pg_get_tabledef(__OID__) || ';'""",
    },
    "SEQUENCE_OWNED_BY": {
        # exploiting (documented) fact, that sequence can be owned by
        # same-schema table only.
        "list": f"""SELECT n.nspname, c.relname as objname, c.oid
        FROM pg_class c2 JOIN pg_depend d2 ON c2.oid=d2.refobjid JOIN pg_attribute a2
          ON (a2.attrelid=c2.oid AND a2.attnum=d2.refobjsubid)
        JOIN pg_class c ON c.oid = d2.objid JOIN pg_namespace n on n.oid = c.relnamespace
        WHERE d2.classid='pg_class'::regclass AND d2.refclassid='pg_class'::regclass
        AND d2.objid = c.oid AND d2.deptype='a' AND c.relkind = 'S'
        AND n.nspname !~ '{IGNORED_SCHEMAS_REGEX}' """,
        "create": """SELECT format('ALTER SEQUENCE %I.%I OWNED BY %I.%I.%I;',
            n.nspname, c.relname, n.nspname, c2.relname, a2.attname)
        FROM pg_class c2 JOIN pg_depend d2 ON c2.oid=d2.refobjid JOIN pg_attribute a2
          ON (a2.attrelid=c2.oid AND a2.attnum=d2.refobjsubid)
        JOIN pg_class c ON c.oid = d2.objid JOIN pg_namespace n on n.oid = c.relnamespace
        WHERE d2.classid='pg_class'::regclass AND d2.refclassid='pg_class'::regclass
        AND d2.objid = c.oid AND d2.deptype='a' AND c.relkind = 'S' AND c.oid = __OID__ """,
    },
    # This is separated from other CONSTRAINTs to work around PK->FK dependency problem.
    "PRIMARY_KEY": {
        "list": f"""SELECT n.nspname, con.conname AS objname, con.oid,
            crn.nspname AS conrelnspname, cr.relname AS conrelname
        FROM pg_constraint con, pg_namespace n, pg_class cr, pg_namespace crn
        WHERE con.contype = 'p'
        AND n.oid = con.connamespace
        AND cr.oid = con.conrelid
        AND crn.oid = cr.relnamespace
        AND n.nspname !~ '{IGNORED_SCHEMAS_REGEX}' """,
        "create": """SELECT format('ALTER TABLE %I.%I ADD CONSTRAINT %I %s;',
            '__CONRELNSPNAME__', '__CONRELNAME__', '__OBJECT__',
            pg_get_constraintdef(__OID__, true))""",
    },
    "CONSTRAINT": {
        "list": f"""SELECT n.nspname, con.conname AS objname, con.oid,
            crn.nspname AS conrelnspname, cr.relname AS conrelname
        FROM pg_constraint con, pg_namespace n, pg_class cr, pg_namespace crn
        WHERE con.contype <> 'p'
        AND n.oid = con.connamespace
        AND cr.oid = con.conrelid
        AND crn.oid = cr.relnamespace
        AND n.nspname !~ '{IGNORED_SCHEMAS_REGEX}' """,
        "create": """SELECT format('ALTER TABLE %I.%I ADD CONSTRAINT %I %s;',
            '__CONRELNSPNAME__', '__CONRELNAME__', '__OBJECT__',
            pg_get_constraintdef(__OID__, true))""",
    },
    "PARTITIONED_INDEX": {
        "list": f"""SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n, pg_index i
        WHERE n.oid = c.relnamespace AND c.relkind = 'I'
        AND i.indexrelid = c.oid AND NOT i.indisprimary
        AND n.nspname !~ '{IGNORED_SCHEMAS_REGEX}'
        AND NOT EXISTS (SELECT 1 FROM pg_constraint con
            WHERE con.conrelid = i.indrelid AND con.conindid = i.indexrelid) """,
        # removing the ONLY keyword to revive partitioned indexes
        "create": """SELECT regexp_replace(pg_get_indexdef(__OID__, 0, true),
            '[[:<:]]ONLY[[:space:]]+', '') || ';'""",
    },
    "INDEX": {
        "list": f"""SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n, pg_index i
        WHERE n.oid = c.relnamespace AND c.relkind = 'i'
        AND i.indexrelid = c.oid AND NOT i.indisprimary
        AND n.nspname !~ '{IGNORED_SCHEMAS_REGEX}'
        AND NOT EXISTS (SELECT 1 FROM pg_constraint con
            WHERE con.conrelid = i.indrelid AND con.conindid = i.indexrelid) """,
        "create": """SELECT pg_get_indexdef(__OID__, 0, true) || ';'""",
    },
    "VIEW": {
        "list": f"""SELECT n.nspname, c.relname AS objname, c.oid
        FROM pg_class c, pg_namespace n
        WHERE n.oid = c.relnamespace AND c.relkind = 'v'
        AND n.nspname !~ '{IGNORED_SCHEMAS_REGEX}'
        AND c.oid NOT IN (SELECT objid FROM pg_depend
                 WHERE refclassid = 'pg_extension'::regclass AND deptype = 'e') """,
        "create": """SELECT E'CREATE VIEW __SCHEMA__.__OBJECT__ AS \\n'
            || pg_get_viewdef(__OID__, true)""",
    },
    "CONTINUOUS_AGGREGATE": {
        "requires": "timescaledb",
        "list": f"""SELECT n.nspname, c.relname AS objname, c.oid
        FROM  timescaledb_information.continuous_aggregates cagg, pg_class c, pg_namespace n
        WHERE n.oid = c.relnamespace AND c.relkind = 'v'
        AND n.nspname !~ '{IGNORED_SCHEMAS_REGEX}'
        AND n.nspname = cagg.view_schema AND c.relname = cagg.view_name
        AND c.oid NOT IN (SELECT objid FROM pg_depend
                 WHERE refclassid = 'pg_extension'::regclass AND deptype = 'e') """,
        "create": """SELECT E'CREATE MATERIALIZED VIEW __SCHEMA__.__OBJECT__ WITH (timescaledb.continuous) AS \\n'
            || view_definition
            FROM timescaledb_information.continuous_aggregates
            WHERE view_schema = '__SCHEMA__' AND view_name = '__OBJECT__' """,
    },
}

logging.basicConfig(
    format="%(asctime)s %(levelname)s %(funcName)s %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    level=logging.INFO,
)

logger = logging.getLogger()


class DatabaseInfo:
    def __init__(self, conn: psycopg2.extensions.connection):
        self.server_version = "Unknown"
        self.extensions = {}
        with conn.cursor() as cur:
            cur.execute("SHOW server_version")
            (self.server_version,) = cur.fetchone()
            cur.execute("SELECT extname, extversion FROM pg_extension")
            for n, v in cur:
                self.extensions[n] = v

    def __str__(self):
        return f"server_version: {self.server_version}, extensions: {self.extensions}"


def main() -> None:

    if ARGS.debug:
        logger.setLevel(logging.DEBUG)

    logger.debug("pgboom %s starting", ARGS.ACTION)

    conn = None
    if ARGS.ACTION != "cat":
        logger.debug("testing database connection")
        try:
            conn = pgconn(ARGS.DSN)
        except psycopg2.Error:
            logger.exception(
                "connection failed. You are supposed to provide connection\n"
                "details in DSN parameter or environment variables [PGHOST, PGDATABASE etc],\n"
                "as specified in www.postgresql.org/docs/current/libpq-envars.html"
            )
            sys.exit(1)

        conninfo = DatabaseInfo(conn)
        logger.debug(f"connection OK: {conninfo}")

    if ARGS.ACTION == "explode":
        res = explode(conn, ARGS.DIR)
    elif ARGS.ACTION == "implode":
        res = implode(conn, ARGS.DIR)
    elif ARGS.ACTION == "diff":
        res = diff(conn, ARGS)
    elif ARGS.ACTION == "cat":
        res = cat(ARGS)
    elif ARGS.ACTION == "test":
        res = test()
    else:
        raise NotImplementedError

    logger.debug("pgboom %s finished, result: %s", ARGS.ACTION, res)


def get_args() -> argparse.Namespace:
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        prog=PROGNAME,
        description=__doc__,
        formatter_class=argparse.RawTextHelpFormatter,
    )

    actiondocs = []
    for a in ACTIONS:
        # grab the docstring from action's method
        adoc = globals()[a].__doc__
        # remove empty lines, usually present due to docstring conventions
        adoc = re.sub(r"\n\s*\n", "\n", adoc, count=1)
        # strip trailing newlines, if present
        adoc = re.sub(r"(\n\s*)+$", "", adoc)
        actiondocs.append(f"{a}: {adoc}\n")
    actionhelp = "\n".join(actiondocs)

    # positional arguments: ACTION DSN DIR
    parser.add_argument("ACTION", choices=ACTIONS, metavar="ACTION", help=actionhelp)
    parser.add_argument("DSN", help="PostgreSQL data source in 'key=value' format")
    parser.add_argument("DIR", help="Destination/source directory")
    # options
    parser.add_argument(
        "--Class",
        "-C",
        choices=OBJECTMETA.keys(),
        help="Type of objects to extract/load",
    )
    parser.add_argument("--Schema", "-S", help="Database schema name regex")
    parser.add_argument("--Object", "-O", help="Database object name regex")
    parser.add_argument("--File", "-F", help="Output file for `cat` and `diff` actions")
    parser.add_argument(
        "--debug",
        "--verbose",
        "-v",
        action="store_true",
        help="Set verbose debugging on",
    )

    return parser.parse_args()


def verify_metadir(adir: str) -> bool:
    """Verifies metadata directory existence and minimal contents."""

    p = pathlib.Path(adir)
    if not (p.exists() and p.is_dir()):
        logger.error("directory %s does not exist", adir)
        sys.exit(2)
    subdirs = [x.name for x in p.iterdir() if x.is_dir()]
    if set(subdirs).isdisjoint(OBJECTMETA.keys()):
        logger.error("directory %s does not look like metadata directory", adir)
        sys.exit(2)
    return True


def explode(conn: psycopg2.extensions.connection, metadir: str) -> dict:
    """Saves object definitions (SQL CREATE statements) from Postgres to directory.

    Each object goes into separate file, DIR/<CLASS>/<schema>/<object>.sql.
    """

    try:
        pathlib.Path(metadir).mkdir(parents=True, exist_ok=True)
    except OSError:
        logger.error("could not access output directory %s", metadir)
        sys.exit(1)

    stats = {}

    extensions = list_extensions(conn)
    for objclass in OBJECTMETA:
        if ARGS.Class is not None and objclass != ARGS.Class:
            continue
        if (
            "requires" in OBJECTMETA[objclass]
            and OBJECTMETA[objclass]["requires"] not in extensions
        ):
            continue
        count = explode_objects(conn, metadir, objclass)
        stats[objclass] = count

    logger.info("finished, stats: %s", stats)
    return stats


def implode(conn: psycopg2.extensions.connection, metadir: str) -> dict:
    """Loads object definitions from directory to Postgres.

    Does not overwrite any pre-existing objects.
    """

    verify_metadir(metadir)
    stats = {}

    for objclass in OBJECTMETA:
        if ARGS.Class is not None and objclass != ARGS.Class:
            continue
        count = implode_objects(conn, metadir, objclass)
        stats[objclass] = count

    logger.info("finished, stats: %s", stats)
    return stats


def diff(conn: psycopg2.extensions.connection, arg) -> int:
    """Compares object definitions between database and directory, using system `diff` command.

    The directory should have same structure as produced by `pgboom explode`.
    Results are saved to --File, in context diff format.
    """

    metadir = arg.DIR
    verify_metadir(metadir)

    if arg.File is None:
        logger.error("The `diff` action requires --File option")
        sys.exit(1)
    try:
        ofd = open(arg.File, mode="w")
    except OSError:
        logger.exception("Cannot open --File")
        sys.exit(1)

    with tempfile.TemporaryDirectory() as dbdir:
        logger.info("exploding database to %s", dbdir)
        explode(conn, dbdir)
        logger.info("comparing %s to %s", dbdir, metadir)
        for objclass in OBJECTMETA:
            dbsubdir = pathlib.Path(dbdir, objclass)
            metasubdir = pathlib.Path(metadir, objclass)
            if dbsubdir.exists() or metasubdir.exists():
                logger.debug("processing %s definitions", objclass)
                subprocess.run(
                    ["diff", "-r", "-C3", "-N", dbsubdir, metasubdir],
                    stdout=ofd,
                    check=False,
                )

        # Return number of bytes written to --File.
        return ofd.tell()


def cat() -> dict:
    """Does the same as implode, but concatenates into --File, not database."""

    if ARGS.File is None:
        raise ValueError('The "cat" action requires --File option.')
    try:
        # overwrite the file
        ofd = open(ARGS.File, mode="w")
        ofd.write("SET check_function_bodies TO off;\n\n")
        ofd.close()
    except OSError:
        logger.exception("Cannot open --File")
        sys.exit(1)
    # implemented as part of implode.
    return implode(None, ARGS.DIR)


def test() -> tuple:
    """Performs a built-in test (for developers)."""

    logger.info(
        "entering test code - please ignore ERROR messages "
        "unless you see specific tests failing"
    )
    return doctest.testmod(verbose=ARGS.debug)


def make_where() -> tuple:
    """Construct a WHERE clause for object list filtering.

    Returns:
        A tuple with two items:
            1. SQL WHERE string with %s-style placeholders
            2. list of parameters for the placeholders.

    >>> save_schema = ARGS.Schema
    >>> save_object = ARGS.Object
    >>> ARGS.Schema = '^myschema$'
    >>> ARGS.Object = '^myobjectprefix'
    >>> print(make_where())
    (' WHERE nspname ~ %s AND objname ~ %s', ['^myschema$', '^myobjectprefix'])
    >>> ARGS.Schema = save_schema
    >>> ARGS.Object = save_object
    """

    conditions = []
    params = []
    clause = ""
    if ARGS.Schema is not None:
        conditions.append("nspname ~ %s")
        params.append(ARGS.Schema)
    if ARGS.Object is not None:
        conditions.append("objname ~ %s")
        params.append(ARGS.Object)
    if conditions:
        clause = " WHERE " + " AND ".join(conditions)
    return (clause, params)


def explode_objects(
    conn: psycopg2.extensions.connection, metadir: str, objclass: str
) -> int:
    """Extract objects of given class from database to flat files.

    Return counter.
    """

    logger.debug("processing %s definitions", objclass)
    count = 0
    with conn.cursor() as cur:
        sqlselect = OBJECTMETA[objclass]["list"]
        sqlfilter, sqlparams = make_where()
        sqlquery = "SELECT * FROM ({}) _lst{} ORDER BY nspname, objname".format(
            sqlselect, sqlfilter
        )
        cur.execute(sqlquery, sqlparams)
        for r in cur:
            if objclass in ["PRIMARY_KEY", "CONSTRAINT"]:
                (nspname, objname, oid, conrelnspname, conrelname) = r
                objdef = getobjdef(
                    conn, objclass, oid, nspname, objname, conrelnspname, conrelname
                )
            else:
                (nspname, objname, oid) = r
                objdef = getobjdef(conn, objclass, oid, nspname, objname)
            fdir = f"{metadir}/{objclass}/{nspname}"
            fname = f"{objname}.sql"
            logger.debug("saving %s/%s", fdir, fname)
            pathlib.Path(fdir).mkdir(parents=True, exist_ok=True)
            with open(f"{fdir}/{fname}", "w") as of:
                of.write(objdef)
            count += 1

    return count or None


def implode_objects(
    conn: psycopg2.extensions.connection, metadir: str, objclass: str
) -> int:
    """Load objects from directory.

    Objects go to database (action=implode) or file (action=cat).

    Returns:
        Counter of processed files.
    """

    # DIR/TABLE
    objdir = pathlib.Path(metadir, objclass)
    if not (objdir.exists() and objdir.is_dir()):
        logger.debug("directory %s does not exist", objdir)
        return None

    count = 0

    logger.debug("processing %s definitions", objclass)

    # DIR/TABLE/public
    for schema_dir in sorted(objdir.iterdir()):
        if not schema_dir.is_dir():
            continue
        schema = schema_dir.name
        if ARGS.Schema and not re.search(ARGS.Schema, schema):
            logger.debug("ignoring %s due to --Schema filter", schema_dir)
            continue
        # DIR/TABLE/public/users.sql
        for objectfile in sorted(schema_dir.glob("*.sql")):
            logger.debug("processing %s", objectfile)
            name = objectfile.stem
            if ARGS.Object and not re.search(ARGS.Object, name):
                logger.debug("ignoring %s due to --Object filter", objectfile)
                continue

            elif ARGS.ACTION == "cat":
                with open(ARGS.File, mode="a") as ofd:
                    ofd.write(objectfile.read_text())
                    count += 1
            elif db_execute_ddl_file(conn, objectfile):
                count += 1

    return count or None


def is_expected_ddl(text: str) -> bool:
    """Try to verify if given text looks like SQL DDL.

    >>> is_expected_ddl('ALTER TABLE x ADD CONSTRAINT x_pkey PRIMARY KEY (id);')
    True
    >>> is_expected_ddl('DROP DATABASE mydb;')
    False
    """

    try:
        text = text.decode()
    except (UnicodeDecodeError, AttributeError):
        pass

    return bool(re.match(r"(CREATE|ALTER TABLE|ALTER SEQUENCE)\b", text))


def db_execute_ddl_file(conn: psycopg2.extensions.connection, filename: str) -> bool:
    """Executes DDL from given file in database.

    File content is only minimally validated: check if first line begins with
    relevant SQL keyword, ie. CREATE or ALTER.

    Args:
        conn: postgres db connection
        filename: path of file to be executed

    Returns:
        True on success, False on failure.

    >>> pg = pgconn(ARGS.DSN)
    >>> tmp1 = tempfile.NamedTemporaryFile(suffix='.sql', buffering=0)
    >>> tmp1.write('SELECT 1;'.encode())
    9
    >>> # Expecting failure:
    >>> db_execute_ddl_file(pg, tmp1.name)
    0
    >>> tmp2 = tempfile.NamedTemporaryFile(suffix='.sql', buffering=0)
    >>> tmp2.write('CREATE TEMP TABLE _testtable (id integer);'.encode())
    42
    >>> # Expecting success:
    >>> db_execute_ddl_file(pg, tmp2.name)
    1
    """

    file = pathlib.Path(filename)

    text = file.read_text()
    if not is_expected_ddl(text):
        logger.error("unexpected content in %s", file)
        logger.debug("unexpected content: %s", text)
        return False
    with conn.cursor() as cur:
        # Explicit BEGIN is needed here, contrary to psycopg2 documentation.
        cur.execute("BEGIN")
        try:
            cur.execute(text)
            status = cur.statusmessage
            if not is_expected_ddl(status):
                raise UserWarning(
                    "Aborting transaction " f'due to "{status}" while executing "{file}"'
                )
        except psycopg2.ProgrammingError as e:
            errcode = psycopg2.errorcodes.lookup(e.pgcode)
            msg = e.diag.message_primary
            # detail = e.diag.message_detail
            # hint = e.diag.message_hint
            logger.error("%s when executing %s: %s", errcode, file, msg)
            return False
        finally:
            cur.execute("END")
    return True


def pgconn(dsn: str = "", autocommit: bool = False) -> psycopg2.extensions.connection:
    """Connects to PostgreSQL.

    Gets a new connection and sets application_name in the session.

    >>> pg = pgconn(ARGS.DSN)
    >>> pgcur = pg.cursor()
    >>> pgcur.execute('SELECT 2+3')
    >>> pgcur.fetchone()
    (5,)
    """

    conn = psycopg2.connect(dsn)
    conn.autocommit = autocommit
    with conn.cursor() as cur:
        cur.execute("SET application_name = %s", (PROGNAME,))
        # revert to standard search path to avoid side effects with
        # pg_get_indexdef and friends.
        cur.execute("SET search_path to public")
        # avoid function validation on implode
        cur.execute("SET check_function_bodies TO off")
        conn.commit()
    return conn


# Following functions exist to grab object definitions. They probably should be
# packaged into separate module.


def getobjdef(
    conn: psycopg2.extensions.connection,
    objclass: str,
    oid: int,
    nspname: str = None,
    objname: str = None,
    conrelnspname: str = None,
    conrelname: str = None,
) -> str:
    """Gets SQL object definition (CREATE statement) from database.

    Either the OID or (nspname, objname) pair will be required.

    Args:
        conn: Pg connection
        objclass: object type (TABLE, FUNCTION, CONSTRAINT etc)
        oid: object OID
        nspname: schema name in database
        objname: unique object name within schema (for functions, this means
            full signature including argument types)
        conrelnspname: referring object schema
        conrelname: referring object name

    Returns:
        Object definition in form of CREATE statement.

    >>> pg = pgconn(ARGS.DSN)
    >>> pgcur = pg.cursor()
    >>> pgcur.execute("create sequence _getobjdef_test"
    ...               " minvalue 1 maxvalue 1000 increment 10 start 1 cycle")
    >>> pgcur.execute("select oid from pg_class where relname='_getobjdef_test'")
    >>> oid = pgcur.fetchone()[0]
    >>> objdef = getobjdef(pg, 'SEQUENCE', oid, 'public', '_getobjdef_test')
    >>> print(objdef)
    CREATE SEQUENCE public._getobjdef_test AS bigint
    INCREMENT 10 MINVALUE 1 MAXVALUE 1000 START 1 CACHE 1 CYCLE;
    <BLANKLINE>
    """
    with conn.cursor() as cur:
        objgen = OBJECTMETA[objclass]["create"]
        objgen = re.sub("__OID__", str(oid), objgen)
        objgen = re.sub("__SCHEMA__", nspname, objgen)
        objgen = re.sub("__OBJECT__", objname, objgen)
        if objclass in ["PRIMARY_KEY", "CONSTRAINT"]:
            objgen = re.sub("__CONRELNSPNAME__", conrelnspname, objgen)
            objgen = re.sub("__CONRELNAME__", conrelname, objgen)
            cur.execute(objgen)
            objdef = cur.fetchone()[0]
        elif objclass in ["PARTITIONED_TABLE", "TABLE"]:
            objdef = gettabledef(conn, oid)
        elif objclass == "AGGREGATE":
            objdef = getaggdef(conn, oid)
        else:
            cur.execute(objgen)
            objdef = cur.fetchone()[0]
    # make sure the definition has trailing newline
    if not re.search(r"\n\Z", objdef):
        objdef += "\n"
    return objdef


def getaggdef(conn: psycopg2.extensions.connection, aggoid: int) -> str:
    """Generate CREATE AGGREGATE statement for given aggregate function.

    Credits for initial SQL go to Erwin Brandstetter
    [https://stackoverflow.com/a/48575430/540341].

    >>> pg = pgconn(ARGS.DSN)
    >>> pgcur = pg.cursor()
    >>> pgcur.execute('''CREATE AGGREGATE _testagg (float8) (
    ...    sfunc = float8_accum, stype = float8[],
    ...    finalfunc = float8_avg, initcond = '{0,0,0}'
    ... )''')
    >>> pgcur.execute("SELECT '_testagg(float8)'::regprocedure::oid")
    >>> tempaggoid = pgcur.fetchone()[0]
    >>> print(getaggdef(pg, tempaggoid))
    CREATE AGGREGATE _testagg(double precision) (
        SFUNC = float8_accum, STYPE = double precision[],
        INITCOND = {0,0,0}, FINALFUNC = float8_avg);
    >>> pgcur.execute('''DROP AGGREGATE _testagg (float8)''')
    """

    aggdefsql = """
SELECT format(E'CREATE AGGREGATE %%s (\n    SFUNC = %%s, STYPE = %%s%%s%%s%%s%%s);'
    , aggfnoid::regprocedure
    , aggtransfn
    , aggtranstype::regtype
    , ', SORTOP = '    || NULLIF(aggsortop, 0)::regoper
    , E',\n    INITCOND = '  || agginitval
    , ', FINALFUNC = ' || NULLIF(aggfinalfn, 0)::regproc
    , CASE WHEN aggfinalextra THEN ', FINALFUNC_EXTRA' END
) AS ddl_agg
FROM pg_aggregate
WHERE aggfnoid = %(oid)s
    """

    with conn.cursor() as cur:
        cur.execute(aggdefsql, {"oid": aggoid})
        defn = cur.fetchone()[0]
        return defn


def gettabledef(conn: psycopg2.extensions.connection, taboid: int) -> str:
    """Generate CREATE TABLE statement for given table.

    >>> pg = pgconn(ARGS.DSN)
    >>> pgcur = pg.cursor()
    >>> pgcur.execute('''CREATE TEMP TABLE gettabledef_testing (
    ...    intcol integer,
    ...    textcol text DEFAULT 'footext')
    ...    ''')
    >>> pgcur.execute("SELECT 'gettabledef_testing'::regclass::oid")
    >>> temptableoid = pgcur.fetchone()[0]
    >>> print(gettabledef(pg, temptableoid)) # doctest: +ELLIPSIS
    CREATE TEMP TABLE pg_temp_....gettabledef_testing (
        intcol integer,
        textcol text DEFAULT 'footext'::text
    );
    """

    with conn.cursor() as cur:
        versql = "SELECT current_setting('server_version_num')::integer"
        cur.execute(versql)
        ver = cur.fetchone()[0]
        if ver >= 120000:
            tabdefsql = """
WITH attrdef AS (
    SELECT
        n.nspname,
        c.relname,
        pg_catalog.array_to_string(c.reloptions || array(select 'toast.' || x from pg_catalog.unnest(tc.reloptions) x), ', ') as relopts,
        c.relpersistence,
        a.attnum,
        a.attname,
        pg_catalog.format_type(a.atttypid, a.atttypmod) as atttype,
        (SELECT substring(pg_catalog.pg_get_expr(d.adbin, d.adrelid, true) for 128) FROM pg_catalog.pg_attrdef d
            WHERE d.adrelid = a.attrelid AND d.adnum = a.attnum AND a.atthasdef) as attdefault,
        a.attnotnull,
        (SELECT c.collname FROM pg_catalog.pg_collation c, pg_catalog.pg_type t
            WHERE c.oid = a.attcollation AND t.oid = a.atttypid AND a.attcollation <> t.typcollation) as attcollation,
        a.attidentity,
        a.attgenerated
    FROM pg_catalog.pg_attribute a
    JOIN pg_catalog.pg_class c ON a.attrelid = c.oid
    JOIN pg_catalog.pg_namespace n ON c.relnamespace = n.oid
    LEFT JOIN pg_catalog.pg_class tc ON (c.reltoastrelid = tc.oid)
    WHERE a.attrelid = %(taboid)s
        AND a.attnum > 0
        AND NOT a.attisdropped
    ORDER BY a.attnum
),
coldef AS (
    SELECT
        attrdef.nspname,
        attrdef.relname,
        attrdef.relopts,
        attrdef.relpersistence,
        pg_catalog.format(
            '%%I %%s%%s%%s%%s%%s',
            attrdef.attname,
            attrdef.atttype,
            case when attrdef.attcollation is null then '' else pg_catalog.format(' COLLATE %%I', attrdef.attcollation) end,
            case when attrdef.attnotnull then ' NOT NULL' else '' end,
            case when attrdef.attdefault is null then ''
                else case when attrdef.attgenerated = 's' then pg_catalog.format(' GENERATED ALWAYS AS (%%s) STORED', attrdef.attdefault)
                    when attrdef.attgenerated <> '' then ' GENERATED AS NOT_IMPLEMENTED'
                    else pg_catalog.format(' DEFAULT %%s', attrdef.attdefault)
                end
            end,
            case when attrdef.attidentity<>'' then pg_catalog.format(' GENERATED %%s AS IDENTITY',
                    case attrdef.attidentity when 'd' then 'BY DEFAULT' when 'a' then 'ALWAYS' else 'NOT_IMPLEMENTED' end)
                else '' end
        ) as col_create_sql
    FROM attrdef
    ORDER BY attrdef.attnum
),
tabdef AS (
    SELECT
        coldef.nspname,
        coldef.relname,
        coldef.relopts,
        coldef.relpersistence,
        string_agg(coldef.col_create_sql, E',\n    ') as cols_create_sql
    FROM coldef
    GROUP BY
        coldef.nspname, coldef.relname, coldef.relopts, coldef.relpersistence
)
SELECT
    format(
        'CREATE%%s TABLE %%I.%%I%%s%%s%%s;',
        case tabdef.relpersistence when 't' then ' TEMP' when 'u' then ' UNLOGGED' else '' end,
        tabdef.nspname,
        tabdef.relname,
        coalesce(
            (SELECT format(E'\n    PARTITION OF %%I.%%I %%s\n', pn.nspname, pc.relname,
                pg_get_expr(c.relpartbound, c.oid))
                FROM pg_class c JOIN pg_inherits i ON c.oid = i.inhrelid
                JOIN pg_class pc ON pc.oid = i.inhparent
                JOIN pg_namespace pn ON pn.oid = pc.relnamespace
                WHERE c.oid = %(taboid)s),
            format(E' (\n    %%s\n)', tabdef.cols_create_sql)
        ),
        case when tabdef.relopts <> '' then format(' WITH (%%s)', tabdef.relopts) else '' end,
        coalesce(E'\nPARTITION BY '||pg_get_partkeydef(%(taboid)s), '')
    ) as table_create_sql
FROM tabdef
            """
        else:
            tabdefsql = """
WITH attrdef AS (
    SELECT
        n.nspname,
        c.relname,
        pg_catalog.array_to_string(c.reloptions || array(select 'toast.' || x from pg_catalog.unnest(tc.reloptions) x), ', ') as relopts,
        c.relpersistence,
        a.attnum,
        a.attname,
        pg_catalog.format_type(a.atttypid, a.atttypmod) as atttype,
        (SELECT substring(pg_catalog.pg_get_expr(d.adbin, d.adrelid, true) for 128) FROM pg_catalog.pg_attrdef d
            WHERE d.adrelid = a.attrelid AND d.adnum = a.attnum AND a.atthasdef) as attdefault,
        a.attnotnull,
        (SELECT c.collname FROM pg_catalog.pg_collation c, pg_catalog.pg_type t
            WHERE c.oid = a.attcollation AND t.oid = a.atttypid AND a.attcollation <> t.typcollation) as attcollation,
        a.attidentity
    FROM pg_catalog.pg_attribute a
    JOIN pg_catalog.pg_class c ON a.attrelid = c.oid
    JOIN pg_catalog.pg_namespace n ON c.relnamespace = n.oid
    LEFT JOIN pg_catalog.pg_class tc ON (c.reltoastrelid = tc.oid)
    WHERE a.attrelid = %(taboid)s
        AND a.attnum > 0
        AND NOT a.attisdropped
    ORDER BY a.attnum
),
coldef AS (
    SELECT
        attrdef.nspname,
        attrdef.relname,
        attrdef.relopts,
        attrdef.relpersistence,
        pg_catalog.format(
            '%%I %%s%%s%%s%%s%%s',
            attrdef.attname,
            attrdef.atttype,
            case when attrdef.attcollation is null then '' else pg_catalog.format(' COLLATE %%I', attrdef.attcollation) end,
            case when attrdef.attnotnull then ' NOT NULL' else '' end,
            case when attrdef.attdefault is null then '' else pg_catalog.format(' DEFAULT %%s', attrdef.attdefault) end,
            case when attrdef.attidentity<>'' then pg_catalog.format(' GENERATED %%s AS IDENTITY',
                    case attrdef.attidentity when 'd' then 'BY DEFAULT' when 'a' then 'ALWAYS' else 'NOT_IMPLEMENTED' end)
                else '' end
        ) as col_create_sql
    FROM attrdef
    ORDER BY attrdef.attnum
),
tabdef AS (
    SELECT
        coldef.nspname,
        coldef.relname,
        coldef.relopts,
        coldef.relpersistence,
        string_agg(coldef.col_create_sql, E',\n    ') as cols_create_sql
    FROM coldef
    GROUP BY
        coldef.nspname, coldef.relname, coldef.relopts, coldef.relpersistence
)
SELECT
    format(
        E'CREATE%%s TABLE %%I.%%I (\n    %%s\n)%%s%%s;',
        case tabdef.relpersistence when 't' then ' TEMP' when 'u' then ' UNLOGGED' else '' end,
        tabdef.nspname,
        tabdef.relname,
        tabdef.cols_create_sql,
        case when tabdef.relopts <> '' then format(' WITH (%%s)',
        tabdef.relopts) else '' end,
        coalesce(E'\nPARTITION BY '||pg_get_partkeydef(%(taboid)s), '')
    ) as table_create_sql
FROM tabdef
            """
        cur.execute(tabdefsql, {"taboid": taboid})
        defn = cur.fetchone()[0]
        return defn


# test-only functions


def test_exp_imp() -> None:
    """Tests database reproduction with explode + implode.

    Uses `pg_dump -s` as comparison method.

    1. define temp names for dbs / files
    >>> import time
    >>> db1 = f'tempdb{int(time.time())}'
    >>> db2 = f'{db1}_exp_imp'
    >>> dsn1 = f'{ARGS.DSN} dbname={db1}'
    >>> dsn2 = f'{ARGS.DSN} dbname={db2}'
    >>> exdir = f'{tempfile.gettempdir()}/{db1}'
    >>> file1 = f'{tempfile.gettempdir()}/{db1}.sql'
    >>> file2 = f'{tempfile.gettempdir()}/{db2}.sql'

    2. create dbs
    >>> conn = pgconn(ARGS.DSN, autocommit=True)
    >>> cur = conn.cursor()
    >>> cur.execute(f'CREATE DATABASE {db1}')
    >>> cur.execute(f'CREATE DATABASE {db2}')
    >>> conn.close()

    3. prepare db1
    >>> conn = pgconn(dsn1, autocommit=True)
    >>> cur = conn.cursor()
    >>> cur.execute('''
    ... CREATE SCHEMA s1
    ... CREATE TABLE t1 (id serial unique, payload text) PARTITION BY RANGE (id)
    ... CREATE INDEX t1_payload_idx ON t1 (payload)
    ... CREATE TABLE t1_pd PARTITION OF t1 DEFAULT
    ... CREATE TABLE t1_p0 PARTITION OF t1 FOR VALUES FROM (0) TO (999999)
    ... CREATE TABLE t2 (id int primary key)
    ... CREATE TABLE t3 (t2id int references t2)
    ... CREATE VIEW v1 AS SELECT id FROM t1 WHERE id > 0
    ... ''')
    >>> cur.execute('''
    ... CREATE FUNCTION f1() RETURNS void LANGUAGE plpgsql AS 'BEGIN END' LEAKPROOF IMMUTABLE
    ... ''')
    >>> conn.close()

    4. explode db1
    >>> conn = pgconn(dsn1)
    >>> explode(conn, exdir) # doctest: +NORMALIZE_WHITESPACE
    {'SCHEMA': 2, 'EXTENSION': None, 'FUNCTION': 1, 'AGGREGATE': None,
     'SEQUENCE': 1, 'PARTITIONED_TABLE': 1, 'TABLE': 4, 'SEQUENCE_OWNED_BY': 1,
     'PRIMARY_KEY': 1, 'CONSTRAINT': 4, 'PARTITIONED_INDEX': 1, 'INDEX': 2, 'VIEW': 1}
    >>> conn.close()

    5. implode db2
    >>> conn = pgconn(dsn2)
    >>> implode(conn, exdir) # doctest: +NORMALIZE_WHITESPACE
    {'SCHEMA': 1, 'EXTENSION': None, 'FUNCTION': 1, 'AGGREGATE': None,
     'SEQUENCE': 1, 'PARTITIONED_TABLE': 1, 'TABLE': 4, 'SEQUENCE_OWNED_BY': 1,
     'PRIMARY_KEY': 1, 'CONSTRAINT': 2, 'PARTITIONED_INDEX': 1, 'INDEX': None, 'VIEW': 1}
    >>> conn.close()

    6. make dumps of both
    >>> proc1 = subprocess.run(['pg_dump', '-s', '-d', dsn1, '-f', file1,
    ...                         '--no-owner', '--no-acl'], check=True)
    >>> proc2 = subprocess.run(['pg_dump', '-s', '-d', dsn2, '-f', file2,
    ...                         '--no-owner', '--no-acl'], check=True)

    7. compare
    >>> import filecmp
    >>> logger.debug('to compare manually, run `diff %s %s', file1, file2)
    >>> filecmp.cmp(file1, file2, shallow=False)
    True

    8. cleanup
    >>> conn = pgconn(ARGS.DSN, autocommit=True)
    >>> cur = conn.cursor()
    >>> cur.execute(f'DROP DATABASE {db1}')
    >>> cur.execute(f'DROP DATABASE {db2}')
    >>> conn.close()
    """


def list_extensions(conn: psycopg2.extensions.connection) -> list:
    extnames = []
    with conn.cursor() as cur:
        cur.execute("SELECT extname FROM pg_extension")
        for r in cur:
            extnames.append(r[0])
    return extnames


if __name__ == "__main__":
    ARGS = get_args()
    main()
